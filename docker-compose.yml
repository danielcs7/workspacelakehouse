services:
  airflow-init:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env
    command: >
      bash -c "
      airflow db migrate &&
      airflow users create --username airflow --firstname Air --lastname Flow --role Admin --email airflow@example.com --password airflow
      "
    volumes:
      - /Volumes/MACBACKUP/workspacelakehouse/notebook/Engineering/data:/opt/airflow/data    
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
      - ./src/airflow/scripts:/opt/airflow/scripts
      - ./src/airflow/data:/opt/airflow/data
      - ./src/airflow/.env:/opt/airflow/.env
    networks:
      - iceber-net

  airflow-webserver:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-webserver
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - .env
    ports:
      - "8080:8080"
    command: webserver
    volumes:
      - ./notebook:/opt/notebook
      - /Volumes/MACBACKUP/workspacelakehouse/notebook/Engineering/data:/opt/airflow/data
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
      - ./src/airflow/.env:/opt/airflow/.env
      - ./src/airflow/scripts:/opt/airflow/scripts
      - ./src/airflow/data:/opt/airflow/data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - iceber-net

  airflow-scheduler:
    image: apache/airflow:2.8.1-python3.10
    container_name: airflow-scheduler
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - .env
    command: scheduler
    volumes:
      - ./notebook:/opt/notebook
      - /Volumes/MACBACKUP/workspacelakehouse/notebook/Engineering/data:/opt/airflow/data    
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
      - ./src/airflow/.env:/opt/airflow/.env
      - ./src/airflow/scripts:/opt/airflow/scripts
      - ./src/airflow/data:/opt/airflow/data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - iceber-net

  # Storages
  postgres:
    container_name: postgres
    hostname: postgres
    image: postgres:11
    ports:
      - "5434:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - ./src/postgres/init-database.sh:/docker-entrypoint-initdb.d/init-database.sh
      - ~/projects/postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "postgres" ]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always
    networks:
      - iceber-net

  trino:
    container_name: trino
    hostname: trino
    image: "trinodb/trino:425"
    restart: always
    ports:
      - "8889:8889"
    volumes:
      - ./src/trino/etc-coordinator:/etc/trino
      - ./src/trino/catalog:/etc/trino/catalog
    depends_on:
      - hive-metastore
    networks:
      - iceber-net

  trino-worker:
    profiles: [ "trino-worker" ]
    container_name: trino-worker
    hostname: trino-worker
    image: "trinodb/trino:425"
    restart: always
    volumes:
      - ./src/trino/etc-worker:/etc/trino
      - ./src/trino/catalog:/etc/trino/catalog
    depends_on:
      - trino
    networks:
      - iceber-net

  minio:
    container_name: minio
    hostname: minio
    image: 'minio/minio'
    ports:
      - '9000:9000'
      - '9001:9001'
    volumes:
      - ./src/minio_data:/data
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${S3_SECRET_KEY}
      MINIO_DOMAIN: ${MINIO_DOMAIN}
    command: server /data --console-address ":9001"
    networks:
      - iceber-net

  minio-job:
    image: 'minio/mc'
    container_name: minio-job
    hostname: minio-job
    env_file:
      - .env
    entrypoint: |
      /bin/bash -c "
      sleep 5;
      /usr/bin/mc config --quiet host add myminio http://minio:9000 \$S3_ACCESS_KEY \$S3_SECRET_KEY || true;
      /usr/bin/mc mb --quiet myminio/datalake || true;
      "
    environment:
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
      AWS_REGION: ${AWS_REGION}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_PATH_STYLE_ACCESS: "true"
    depends_on:
      - minio
    networks:
      - iceber-net

  hive-metastore:
    container_name: hive-metastore
    hostname: hive-metastore
    build:
      dockerfile: ./src/hive-metastore/Dockerfile
    image: dataincode/openlakehouse:hive-metastore-3.1.2
    env_file:
      - .env
    ports:
      - '9083:9083'
    environment:
      HIVE_METASTORE_DRIVER: org.postgresql.Driver
      HIVE_METASTORE_JDBC_URL: ${HIVE_METASTORE_JDBC_URL}
      HIVE_METASTORE_USER: hive
      HIVE_METASTORE_PASSWORD: hive
      HIVE_METASTORE_WAREHOUSE_DIR: ${HIVE_METASTORE_WAREHOUSE_DIR}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      S3_PATH_STYLE_ACCESS: "true"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - iceber-net

  spark-iceberg:
    build:
      dockerfile: ./src/spark/Dockerfile-spark3.5
    image: dataincode/openlakehouse:spark-3.5
    container_name: spark-iceberg
    hostname: spark-iceberg
    entrypoint: |
      /bin/bash -c "
      /opt/spark/sbin/start-master.sh &&
      jupyter lab --notebook-dir=/opt/notebook --ip='*' --NotebookApp.token='' --NotebookApp.password='' --port=8888 --no-browser --allow-root
      "
    ports:
      - "4040:4040"
      - "8900:8888"
      - "7077:7077"  # Adicionado para o mestre Spark
      - "8082:8080"  # Porta da UI do mestre Spark
    depends_on:
      - minio
      - hive-metastore
    environment:
      SPARK_MODE: master
      SPARK_MASTER_MEMORY: 4g
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
      AWS_REGION: ${AWS_REGION}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_PATH_STYLE_ACCESS: "true"
    volumes:
      - ./notebook:/opt/notebook
      - ./src/jupyter/jupyter_server_config.py:/root/.jupyter/jupyter_server_config.py
      - ./src/jupyter/themes.jupyterlab-settings:/root/.jupyter/lab/user-settings/@jupyterlab/apputils-extension/themes.jupyterlab-settings
      - ./src/spark/spark-defaults-iceberg.conf:/opt/spark/conf/spark-defaults.conf
      - ./src/spark/spark-env.sh:/opt/spark/conf/spark-env.sh
    networks:
      - iceber-net

  spark-worker:
    build:
      dockerfile: ./src/spark/Dockerfile-spark3.5
    image: dataincode/openlakehouse:spark-3.5
    deploy:
      replicas: 1
    hostname: spark-worker-{{.Task.Slot}}
    entrypoint: |
      /bin/bash -c "
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-iceberg:7077
      "
    depends_on:
      - spark-iceberg
      - minio
      - hive-metastore
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_CORES: 2
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${S3_SECRET_KEY}
      AWS_REGION: ${AWS_REGION}
      AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_PATH_STYLE_ACCESS: "true"
    volumes:
      - ./notebook:/opt/notebook
      - ./src/spark/spark-defaults-iceberg.conf:/opt/spark/conf/spark-defaults.conf
      - ./src/spark/spark-env.sh:/opt/spark/conf/spark-env.sh
    networks:
      - iceber-net

# Configure Network
networks:
  iceber-net:
    name: iceber-net